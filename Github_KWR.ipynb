{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github KWR",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-vdr/streamlit/blob/main/Github_KWR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpiUu9i2BEO"
      },
      "source": [
        "#**Make a Keyword Research in 5 minutes using Google Suggest**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fku_9yeYHywm"
      },
      "source": [
        "This Keyword Script will quickly generate keyword ideas from Google Suggest and cluster them using the most common words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRKmt_iyH-Lz"
      },
      "source": [
        "## Step 1: Change Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfsqSDfgw4Dx"
      },
      "source": [
        "Fill in the field lang_code with your language code (e.g. en, fr, es, nl) and enter up to 5 seed keywords you want to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAIgPfwv2BRf"
      },
      "source": [
        "#language code and keywords\n",
        "lang_code=\"en\"#@param {type:\"string\"}\n",
        "keyword1=\"dog food\" #@param {type:\"string\"}\n",
        "keyword2=\"cat food\" #@param {type:\"string\"}\n",
        "keyword3=\"\" #@param {type:\"string\"}\n",
        "keyword4=\"\" #@param {type:\"string\"}\n",
        "keyword5=\"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQhsBjlvIA17"
      },
      "source": [
        "## Step 2: Run The Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkGuaevewY-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f82152-158b-4052-8d9d-b696d4d18482"
      },
      "source": [
        "#generate keyword list\n",
        "keywords=[keyword1,keyword2,keyword3,keyword4,keyword5]\n",
        "keywordlist = list(filter(None, keywords))\n",
        "keywordlist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog food', 'cat food']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56wYjASE4moS"
      },
      "source": [
        "### Importeer modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK164hka4n6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4d7f99-bf63-4784-afc5-ec58316d9c6f"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "pip install stop_words\n",
        "from stop_words import get_stop_words\n",
        "from google.colab import files\n",
        "%load_ext google.colab.data_table\n",
        "from collections import Counter\n",
        "from json import loads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp37-none-any.whl size=32913 sha256=a816907a8972dc87bca1d76b801be7ae8e43e1583040facf82907084b236aeec\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJD88SfatgOF"
      },
      "source": [
        "#Make a list of letters to use for Google Suggest\n",
        "letterlist=[\"\"]\n",
        "letterlist=letterlist+list(string.ascii_lowercase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0OtLNIV9CZa"
      },
      "source": [
        "#Google Suggest for each combination of keyword and letter\n",
        "keywordsuggestions=[]\n",
        "for keyword in keywordlist: \n",
        "  for letter in letterlist :\n",
        "    URL=\"http://suggestqueries.google.com/complete/search?client=firefox&hl=\"+str(lang_code)+\"&q=\"+keyword+\" \"+letter\n",
        "    headers = {'User-agent':'Mozilla/5.0'} \n",
        "    response = requests.get(URL, headers=headers) \n",
        "    result = json.loads(response.content.decode('utf-8'))\n",
        "    keywordsuggest=[keyword,letter] \n",
        "    for word in result[1]:\n",
        "      if(word!=keyword):\n",
        "        keywordsuggest.append(word)\n",
        "    time.sleep(1)\n",
        "    keywordsuggestions.append(keywordsuggest)\n",
        "#crearte a dataframe from this list\n",
        "keywordsuggestions_df = pd.DataFrame(keywordsuggestions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vffQL_3HBnci"
      },
      "source": [
        "#Rename columns of dataframe\n",
        "columnnames=[\"Keyword\",\"Letter\"]\n",
        "for i in range(1,len(keywordsuggestions_df.columns)-1):\n",
        "  columnnames.append(\"Suggestion\"+str(i))\n",
        "keywordsuggestions_df.columns=columnnames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHfHcVT3xtwF"
      },
      "source": [
        "#Make a list of all suggestions\n",
        "allkeywords = keywordlist\n",
        "for i in range(1,len(keywordsuggestions_df.columns)-1):\n",
        "  suggestlist = keywordsuggestions_df[\"Suggestion\"+str(i)].values.tolist()\n",
        "  for suggestion in suggestlist:\n",
        "    allkeywords.append(suggestion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnT2D89MVNcO"
      },
      "source": [
        "#exclude stopwords and seed keywords from this list\n",
        "stop_words=get_stop_words(lang_code)\n",
        "wordlist=[]\n",
        "seed_words=[]\n",
        "for keyword in keywords:\n",
        "   for seed_word in nltk.word_tokenize(str(keyword).lower()):\n",
        "     if(len(seed_word)>0):\n",
        "       seed_words.append(seed_word)\n",
        "for keyword in allkeywords:\n",
        "   words = nltk.word_tokenize(str(keyword).lower()) \n",
        "   #word tokenizer\n",
        "   for word in words:\n",
        "     if(word not in stop_words and word not in seed_words and len(word)>1):\n",
        "      wordlist.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybcpyh7ABAi"
      },
      "source": [
        "#find the most common words in the suggestions\n",
        "most_common_words= [word for word, word_count in Counter(wordlist).most_common(200)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lngt-nq7KTcd"
      },
      "source": [
        "#assign each suggestion to a common keyword\n",
        "clusters=[]\n",
        "for common_word in most_common_words:\n",
        "    for keyword in allkeywords:\n",
        "      if(common_word in str(keyword)):\n",
        "         clusters.append([keyword,common_word])\n",
        "clusterdf = pd.DataFrame(clusters,columns=['Keyword', 'Cluster'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WokbT7YvJYCC"
      },
      "source": [
        "## Step 3: End Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6M_We-qGvtl"
      },
      "source": [
        "#create dataframe wiht clusters en suggestions\n",
        "clusterdf.to_csv(\"keywords_clustered.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}